{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Using mt5-base for translation\n",
        "\n",
        "Polish-->Japanese based on the Tatoeba dataset, fine-tuning with LoRA\n",
        "\n",
        "Dataset used:\n",
        "\n",
        "[Tatoeba](https://opus.nlpl.eu/Tatoeba/corpus/version/Tatoeba)\n",
        "\n",
        "Citations: J. Tiedemann, 2012, [Parallel Data, Tools and Interfaces in OPUS](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf). In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)"
      ],
      "metadata": {
        "id": "665VryhxA-BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "uU0Ro-EUBnmn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12tHvhbhA0M5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate scikit-learn peft -Uqq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Data from the Tatoeba project, split up and converted to HF dataset format\n",
        "# For other datasets remember to shuffle! This one already is shuffled\n",
        "dataset_train = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Datasets/Tatoeba_train.json\")\n",
        "dataset_valid = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Datasets/Tatoeba_valid.json\")\n",
        "dataset_test = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Datasets/Tatoeba_test.json\")\n"
      ],
      "metadata": {
        "id": "QQ6JtTbcBPfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/valid/test split"
      ],
      "metadata": {
        "id": "oQ_lzL7tCMqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "ds_splits = DatasetDict({\n",
        "    'train': dataset_train['train'],\n",
        "    'valid': dataset_valid['train'],\n",
        "    'test': dataset_test[\"train\"]\n",
        "})"
      ],
      "metadata": {
        "id": "b9Anh1xBCPyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vrCD2pdCf1_",
        "outputId": "45062fb7-2c86-4083-cf1d-861910fead19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Source', 'Target'],\n",
              "        num_rows: 22350\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['Source', 'Target'],\n",
              "        num_rows: 1242\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Source', 'Target'],\n",
              "        num_rows: 1242\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_splits[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrjQ2B8RQl55",
        "outputId": "d46ed331-9ce0-4ebf-f4d6-9eeade92fe59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Source': 'Dlaczego powiedziałeś coś tak głupiego?',\n",
              " 'Target': 'どうしてそんなに馬鹿なことを言ったの？'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU availability"
      ],
      "metadata": {
        "id": "m3BQqdfy9OIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"CUDA available. Device count:\")\n",
        "  print(torch.cuda.device_count())\n",
        "  device_id = torch.cuda.current_device()\n",
        "  print(torch.cuda.get_device_name(device_id))\n",
        "else:\n",
        "  print(\"CUDA unavailable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9nb7qQ09P0r",
        "outputId": "9541dc0b-28ec-4e01-a22e-068f854800de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available. Device count:\n",
            "1\n",
            "NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the model and wrap it in the peft object"
      ],
      "metadata": {
        "id": "611cvZHfEuXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, MT5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
        "original_model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX3CJpbNE2Wx",
        "outputId": "d292152a-6f04-4753-e645-a9f0884e986f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "mGQymdgjExKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(original_model,\n",
        "                            lora_config)"
      ],
      "metadata": {
        "id": "Zjn4io5IE7wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "gcVvRe2nF-_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c778acf-49b1-4892-e3b7-6851abfd2755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,538,944 || all params: 585,940,224 || trainable%: 0.6040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "Tfz1GZxvF_e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db661350-f8a6-4e5b-bf45-7803068abdcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForSeq2SeqLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): MT5ForConditionalGeneration(\n",
            "      (shared): Embedding(250112, 768)\n",
            "      (encoder): MT5Stack(\n",
            "        (embed_tokens): Embedding(250112, 768)\n",
            "        (block): ModuleList(\n",
            "          (0): MT5Block(\n",
            "            (layer): ModuleList(\n",
            "              (0): MT5LayerSelfAttention(\n",
            "                (SelfAttention): MT5Attention(\n",
            "                  (q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (relative_attention_bias): Embedding(32, 12)\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (1): MT5LayerFF(\n",
            "                (DenseReluDense): MT5DenseGatedActDense(\n",
            "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (act): NewGELUActivation()\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1-11): 11 x MT5Block(\n",
            "            (layer): ModuleList(\n",
            "              (0): MT5LayerSelfAttention(\n",
            "                (SelfAttention): MT5Attention(\n",
            "                  (q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (1): MT5LayerFF(\n",
            "                (DenseReluDense): MT5DenseGatedActDense(\n",
            "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (act): NewGELUActivation()\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): MT5LayerNorm()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (decoder): MT5Stack(\n",
            "        (embed_tokens): Embedding(250112, 768)\n",
            "        (block): ModuleList(\n",
            "          (0): MT5Block(\n",
            "            (layer): ModuleList(\n",
            "              (0): MT5LayerSelfAttention(\n",
            "                (SelfAttention): MT5Attention(\n",
            "                  (q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (relative_attention_bias): Embedding(32, 12)\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (1): MT5LayerCrossAttention(\n",
            "                (EncDecAttention): MT5Attention(\n",
            "                  (q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (2): MT5LayerFF(\n",
            "                (DenseReluDense): MT5DenseGatedActDense(\n",
            "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (act): NewGELUActivation()\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1-11): 11 x MT5Block(\n",
            "            (layer): ModuleList(\n",
            "              (0): MT5LayerSelfAttention(\n",
            "                (SelfAttention): MT5Attention(\n",
            "                  (q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (1): MT5LayerCrossAttention(\n",
            "                (EncDecAttention): MT5Attention(\n",
            "                  (q): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                  (v): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (2): MT5LayerFF(\n",
            "                (DenseReluDense): MT5DenseGatedActDense(\n",
            "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
            "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (act): NewGELUActivation()\n",
            "                )\n",
            "                (layer_norm): MT5LayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): MT5LayerNorm()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the tokenizer"
      ],
      "metadata": {
        "id": "D9JpiWFiKh5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokenizer(input_text):\n",
        "  input_tokenized = tokenizer(input_text, return_tensors=\"pt\")\n",
        "  print(input_tokenized)\n",
        "  out = tokenizer.decode(input_tokenized.input_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "  print(f\"In: {input_text}\")\n",
        "  print(f\"Out: {out}\")\n",
        "\n",
        "test_tokenizer(\"Samochód\")\n",
        "test_tokenizer(\"Chodźmy do żabki\")\n",
        "test_tokenizer(\"ザブカへ行きましょう\")"
      ],
      "metadata": {
        "id": "7U9jzQ3HKi51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3222a0b8-8801-4ca2-cc2a-e256bec77bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[22115, 55337,   285,     1]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
            "In: Samochód\n",
            "Out: Samochód\n",
            "{'input_ids': tensor([[  8144,  15732,   1813,    342,  50478, 111528,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "In: Chodźmy do żabki\n",
            "Out: Chodźmy do żabki\n",
            "{'input_ids': tensor([[  259, 16786, 11594,  6388,  6031, 68222, 46265,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "In: ザブカへ行きましょう\n",
            "Out: ザブカへ行きましょう\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize"
      ],
      "metadata": {
        "id": "xUWg3rbiKF8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [f\"Translate Polish to Japanese: {source_text}\" for source_text in examples[\"Source\"]]\n",
        "    targets = examples[\"Target\"]\n",
        "\n",
        "    # Tokenize inputs and outputs\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding='max_length')\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Preprocess the dataset\n",
        "tokenized_dataset = ds_splits.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "UCT99T03KGr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "p356GAtIKK6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86dc7b2-10dc-4356-b3a9-c43a806d8b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Source': 'Dlaczego powiedziałeś coś tak głupiego?',\n",
              " 'Target': 'どうしてそんなに馬鹿なことを言ったの？',\n",
              " 'input_ids': [89349,\n",
              "  259,\n",
              "  58459,\n",
              "  288,\n",
              "  30865,\n",
              "  267,\n",
              "  259,\n",
              "  30104,\n",
              "  22099,\n",
              "  259,\n",
              "  58942,\n",
              "  78179,\n",
              "  964,\n",
              "  3376,\n",
              "  756,\n",
              "  259,\n",
              "  318,\n",
              "  82729,\n",
              "  52770,\n",
              "  291,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'labels': [259,\n",
              "  149845,\n",
              "  125065,\n",
              "  161555,\n",
              "  227583,\n",
              "  6961,\n",
              "  423,\n",
              "  291,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "br5SQYTHKNb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi[unidic-lite]"
      ],
      "metadata": {
        "id": "cKCsVI7YcyxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e84fd0-801d-4031-925a-c16494db4414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fugashi[unidic-lite] in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: unidic-lite in /usr/local/lib/python3.11/dist-packages (from fugashi[unidic-lite]) (1.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "tagger = Tagger('-Owakati')\n",
        "def tokenize_japanese(text):\n",
        "  return [word.surface for word in tagger(text)]"
      ],
      "metadata": {
        "id": "fJGPfvjSclpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"麩菓子は、麩を主材料とした日本の菓子。\"\n",
        "tokenize_japanese(text)"
      ],
      "metadata": {
        "id": "0i2YS8jGe_uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3041a8a5-7d45-433e-ef5a-904be497bb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['麩', '菓子', 'は', '、', '麩', 'を', '主材', '料', 'と', 'し', 'た', '日本', 'の', '菓子', '。']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "tdwvVBeFfhSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa92a71-5b9b-4838-ae3f-664ba95d1390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=4e-4,\n",
        "    per_device_train_batch_size=32, # 32 or 16--> OOM, 8 was fine on a T4\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy = \"epoch\",\n",
        "    metric_for_best_model='eval_loss',\n",
        "    predict_with_generate=True\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"valid\"],\n",
        "    callbacks=[EarlyStoppingCallback(3, 0.0)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_9ZGW_KSKOPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f2b9d995-f1cc-4088-b2bb-91eb491364ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2796' max='2796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2796/2796 25:09, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.351600</td>\n",
              "      <td>0.260545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.303600</td>\n",
              "      <td>0.222252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.282600</td>\n",
              "      <td>0.213637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>0.211120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2796, training_loss=1.3506807056875187, metrics={'train_runtime': 1510.6134, 'train_samples_per_second': 59.181, 'train_steps_per_second': 1.851, 'total_flos': 2.7041662107648e+16, 'train_loss': 1.3506807056875187, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "save_time = datetime.now()\n",
        "save_time_str = save_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_dir = f\"mt5-base-pl-ja-adapter-{save_time_str}\"\n",
        "print(\"Saving the model\")\n",
        "model.save_pretrained(save_dir)"
      ],
      "metadata": {
        "id": "zrziazc_KQ2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef3aba5-ee48-4004-ed38-c9c4cc3295c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "zip_filename = f\"{save_dir}.zip\"\n",
        "drive_path = f\"/content/drive/MyDrive/Models/{zip_filename}\"\n",
        "print(\"Zipping the model\")\n",
        "os.system(f\"zip -r {zip_filename} {save_dir}\")"
      ],
      "metadata": {
        "id": "iLDNjJ7dxZDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a876265-3fec-4d44-e713-7b5c00817a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping the model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(f\"mv {zip_filename} '{drive_path}'\")"
      ],
      "metadata": {
        "id": "iglvQHOFdsx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea5fb32-de3d-4ee6-e2b9-dd378590fbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model moved to google drive!\")"
      ],
      "metadata": {
        "id": "nFJNwuip3zxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4661b608-ddf7-461b-ad89-3842e8d0db89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model moved to google drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model filename:\")\n",
        "print(zip_filename)"
      ],
      "metadata": {
        "id": "FTy6Pcvc31x7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ab4966-e628-497b-9697-3bdacc0b9ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model filename:\n",
            "mt5-base-pl-ja-adapter-2025-02-12_15-01-23.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ],
      "metadata": {
        "id": "4YGA_ub9KcDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_dataset[\"train\"][\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "xSSHMvi7zbPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6b90d33f-e548-416a-fbde-c63c2b17bb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Translate Polish to Japanese: Dlaczego powiedziałeś coś tak głupiego?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_dataset[\"train\"][\"labels\"][0])"
      ],
      "metadata": {
        "id": "yf3gp6B2zvKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "994781dd-c8dd-4593-fcfe-4d692851d3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'どうしてそんなに馬鹿なことを言ったの?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def text_to_translation_prompt(source_text):\n",
        "  return f\"Translate Polish to Japanese: {source_text}\"\n",
        "\n",
        "def translate_text(source_text, temperature=0.3, top_k=20):\n",
        "  input_text = text_to_translation_prompt(source_text)\n",
        "  input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "  with torch.no_grad():\n",
        "    output_ids = model.generate(input_ids=input_ids,\n",
        "                                top_k=top_k,\n",
        "                                temperature = temperature,\n",
        "                                do_sample=True)\n",
        "  print(f\"PL: {source_text}\")\n",
        "  print(f\"JP: {tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)}\")\n",
        "  print(\"---\")\n",
        "\n",
        "texts_to_translate = [\n",
        "    \"Pogasiła wszystkie światła o dziesiątej.\"\n",
        "    \"Chodźmy do żabki\",\n",
        "    \"Chodźmy do kina\",\n",
        "    \"Spokojnie jak na wojnie\",\n",
        "    \"lol\",\n",
        "    \"Wiedźmin to super gra\",\n",
        "    \"Lubię programować\",\n",
        "    \"Polski to trudny język\",\n",
        "    \"Kalendarz Gregoriański został wprowadzony w 1582 roku\",\n",
        "    \"Nie można oczekiwać świetnych wyników od modelu który nie uczył się nawet na całym zbiorze danych\",\n",
        "    \"Test test test\",\n",
        "    \"Jestem głodny\"\n",
        "]\n",
        "\n",
        "for text in texts_to_translate:\n",
        "  translate_text(text)"
      ],
      "metadata": {
        "id": "EXlywlvwKc6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eece6ab3-a161-4308-a8a9-66c5243826ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PL: Pogasiła wszystkie światła o dziesiątej.Chodźmy do żabki\n",
            "JP: 彼女はすべての光を雨に焚いた。\n",
            "---\n",
            "PL: Chodźmy do kina\n",
            "JP: 私たちは映画に行こう。\n",
            "---\n",
            "PL: Spokojnie jak na wojnie\n",
            "JP: 戦争中だ。\n",
            "---\n",
            "PL: lol\n",
            "JP: 笑う。\n",
            "---\n",
            "PL: Wiedźmin to super gra\n",
            "JP: ミミは素晴らしいゲームだ。\n",
            "---\n",
            "PL: Lubię programować\n",
            "JP: プログラムが好きだ。\n",
            "---\n",
            "PL: Polski to trudny język\n",
            "JP: 日本語は難しすぎる。\n",
            "---\n",
            "PL: Kalendarz Gregoriański został wprowadzony w 1582 roku\n",
            "JP: Gregoriのカレンダーは1582年改められた。\n",
            "---\n",
            "PL: Nie można oczekiwać świetnych wyników od modelu który nie uczył się nawet na całym zbiorze danych\n",
            "JP: そのモデルは全部データに理解できなかった。\n",
            "---\n",
            "PL: Test test test\n",
            "JP: テストテストテストを英語で翻訳した。\n",
            "---\n",
            "PL: Jestem głodny\n",
            "JP: 疲れている。\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_text(\"samochód\")"
      ],
      "metadata": {
        "id": "Z3UYhgsXKfGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380867fb-3684-4316-808e-73a456855cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PL: samochód\n",
            "JP: 車は運転手だ。\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts_to_translate = [\n",
        "    \"Test test test\",\n",
        "    \"Mam na imię Adrian\",\n",
        "    \"Tłumaczenie jest trudne\",\n",
        "    \"Mają ulubioną potrawą jest omlet\",\n",
        "    \"Tom ma bardzo szybki samochód\",\n",
        "    \"Samochód Toma jest bardzo szybki\"\n",
        "]\n",
        "\n",
        "for text in texts_to_translate:\n",
        "  translate_text(text)"
      ],
      "metadata": {
        "id": "sDiGC5o5GSPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ac7560-4cf6-4506-88d0-648024bc8946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PL: Test test test\n",
            "JP: テストテストテストをテストした。\n",
            "---\n",
            "PL: Mam na imię Adrian\n",
            "JP: 私の名前はAdrianです。\n",
            "---\n",
            "PL: Tłumaczenie jest trudne\n",
            "JP: 翻訳は難しそう。\n",
            "---\n",
            "PL: Mają ulubioną potrawą jest omlet\n",
            "JP: 彼らはお気に入りの料理はオリーブです。\n",
            "---\n",
            "PL: Tom ma bardzo szybki samochód\n",
            "JP: トムは速い車を持っている。\n",
            "---\n",
            "PL: Samochód Toma jest bardzo szybki\n",
            "JP: トムは速い。\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts_to_translate = [\n",
        "    \"Zagrajmy w grę\",\n",
        "    \"Poczekaj chwilę!\",\n",
        "    \"Nie wiem co zrobić\",\n",
        "    \"Gdzie jest stacja kolejowa?\",\n",
        "    \"Jak dojść na stację kolejową?\",\n",
        "    \"Smutno mi\",\n",
        "    \"Cicho bądź!\"\n",
        "]\n",
        "\n",
        "for text in texts_to_translate:\n",
        "  translate_text(text)"
      ],
      "metadata": {
        "id": "io71Ap8mItWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24198575-7606-412b-a1b5-5948be969098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PL: Zagrajmy w grę\n",
            "JP: ゲームをプレイしましょう。\n",
            "---\n",
            "PL: Poczekaj chwilę!\n",
            "JP: 時間があったら待つ。\n",
            "---\n",
            "PL: Nie wiem co zrobić\n",
            "JP: 何をすればいいか分からない。\n",
            "---\n",
            "PL: Gdzie jest stacja kolejowa?\n",
            "JP: 電車の駅はどこですか。\n",
            "---\n",
            "PL: Jak dojść na stację kolejową?\n",
            "JP: 列車に行きますか。\n",
            "---\n",
            "PL: Smutno mi\n",
            "JP: とても悲しく。\n",
            "---\n",
            "PL: Cicho bądź!\n",
            "JP: あなたは、いいよ。\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in texts_to_translate:\n",
        "  translate_text(text, temperature=1, top_k=100)"
      ],
      "metadata": {
        "id": "4IiO7EvGJiZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e34024-fa25-4bda-a8f9-c520f8143ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PL: Zagrajmy w grę\n",
            "JP: ゲームをスタートしましょう。\n",
            "---\n",
            "PL: Poczekaj chwilę!\n",
            "JP: しばらく待ってくれ。\n",
            "---\n",
            "PL: Nie wiem co zrobić\n",
            "JP: 何すればいいのよ。\n",
            "---\n",
            "PL: Gdzie jest stacja kolejowa?\n",
            "JP: 電車の近くの駅はどこですか。\n",
            "---\n",
            "PL: Jak dojść na stację kolejową?\n",
            "JP: なぜ新幹線に乗って帰ったかを知りました。\n",
            "---\n",
            "PL: Smutno mi\n",
            "JP: 私には緊張していると私が悪い。\n",
            "---\n",
            "PL: Cicho bądź!\n",
            "JP: 色いいですね。\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the BLEU score"
      ],
      "metadata": {
        "id": "CvoNSItead-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions = []\n",
        "batch_size = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_input_ids = tokenized_dataset['test'][\"input_ids\"]\n",
        "true_translations = tokenized_dataset['test'][\"Target\"]\n",
        "\n",
        "for i in range(0, len(test_input_ids), batch_size):\n",
        "    batch_input_ids = test_input_ids[i:i + batch_size]\n",
        "\n",
        "    input_ids_tensor = torch.tensor(batch_input_ids).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids=input_ids_tensor)\n",
        "\n",
        "    batch_predictions = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False) for output in output_ids]\n",
        "    model_predictions.extend(batch_predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5J29wVCPagNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions[0]"
      ],
      "metadata": {
        "id": "0XgnFVDi2UWk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61f8eebd-40df-4d78-d05b-cd81f4beefc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'明日、この場所に行ってきます。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model_predictions)"
      ],
      "metadata": {
        "id": "kd-6mTf03O1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8af7ec-70db-43bf-c551-8548d277da01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1242"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "n8k60y2zd_6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95578692-4d9c-4e08-a2de-9b953bcac35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fugashi\n",
        "tagger = fugashi.Tagger()\n",
        "\n",
        "tagger = Tagger('-Owakati')\n",
        "def tokenize_japanese(text):\n",
        "  return [word.surface for word in tagger(text)]\n",
        "\n",
        "tokenized_target = []\n",
        "tokenized_predictions = []\n",
        "\n",
        "for text in tokenized_dataset['test'][\"Target\"]:\n",
        "    tokenized_target.append([tokenize_japanese(text)]) # Single reference"
      ],
      "metadata": {
        "id": "BPOiO2O_czfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in model_predictions:\n",
        "    tokenized_predictions.append(tokenize_japanese(text))"
      ],
      "metadata": {
        "id": "TXumIThsd-AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset['test'][\"Target\"][0]"
      ],
      "metadata": {
        "id": "QEDSgBFo2z7r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30991382-ff74-4a03-dfcf-2e04a3d37ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'明日の今頃は大阪を見物しているでしょう。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_target[0]"
      ],
      "metadata": {
        "id": "MLBrRN3IfJnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186d0223-559d-4afb-ee46-e9602dfc0ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['明日', 'の', '今頃', 'は', '大阪', 'を', '見物', 'し', 'て', 'いる', 'でしょう', '。']]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_predictions[0]"
      ],
      "metadata": {
        "id": "iRnzPYcqfLD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559779f3-8aaa-4e3d-d0c5-2f8298195379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['明日', '、', 'この', '場所', 'に', '行っ', 'て', 'き', 'ます', '。']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_target)"
      ],
      "metadata": {
        "id": "E1xNKYhm3AtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fcf1aa-e536-49a2-f2ef-cb0595ae6573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1242"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_predictions)"
      ],
      "metadata": {
        "id": "rdwNJII63Czc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ee1dee-fe67-4b32-cf5e-fd0fc1d761ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1242"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.translate.bleu_score.corpus_bleu(tokenized_target, tokenized_predictions)"
      ],
      "metadata": {
        "id": "F2_webf-d73l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372a2014-d363-48d3-8597-7de3f4b0e396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12440461525629558"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}